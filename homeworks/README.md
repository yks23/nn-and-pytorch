# 简单小实验
## 实验零
https://playground.tensorflow.org/
不用写代码的活，试试把这个网站上的按钮都按一按。
## 实验一
本部分包含三个小实验，希望通过这些实验，帮助你对最基本的超参有所了解。
参看mnist.ipynb，运行其中的代码，观察不同超参对模型训练速度和最终性能的影响。
mnist，梦的开始。
## 实验二
参看sin.ipynb，这个实验是用一个多项式函数拟合sin(x)函数。
探讨以下问题：
- 为什么batchsize小的时候，模型训练很不稳定
- 为什么预测超出训练范围的点时，模型会出现很大的误差
- 如果要增加模型能够学习的范围，应该怎么改进
## 实验三（Optional，for those who have access to gpus）
本地部署一个文生图模型来玩玩！（让我们假设你的电脑有显卡，多小无所谓，总能找到合适的）
推荐的路径：
- 本地安装Nvidia相关工具
- nvidia-smi查看显卡信息
- 根据显卡信息，去huggingface上找到**合适大小**的模型
- 下载模型
- 运行模型，生成图片

这个过程中，你需要学会：
    - 根据显卡大小，估算可以最大运行的模型（calculating）
    - 如何下载huggingface上的模型（downloading）
    - 如何使用huggingface的模型生成图片（infering）

实际上，这些技能都是非常有用的。我们强烈推荐您在**OpenAI/Claude**等工具的帮助下，完成这一小节的任务！
每个环节你都可能会遇到问题，没关系，使用**OpenAI/Claude**等工具来帮助你解决问题。
可能但不限于：
- huggingface下载速度慢
- 模型太大，显存不够
- huggingface连不上/无权限
...
## 实验四 
估算模型的训练/推理时显存！


在训练神经网络时，内存主要消耗在：  
1. **模型参数**（Parameters）：所有权重和偏置的存储。  
2. **中间激活值**（Activations）：前向传播时每层的输出值（需保存用于反向传播）。  
3. **梯度**（Gradients）：反向传播时计算的梯度。  

推理时内存占用公式（简化版）：  
\[
\text{总内存} \approx \text{模型参数内存} + \text{激活值内存} + \text{梯度内存}
\]  
其中：  
- 模型参数内存 = `参数量 × 每个参数字节数`（通常float32占4字节）。  
- 激活值内存 ≈ `batch size × 每样本激活值数量 × 4字节`。  
- 梯度内存 ≈ 模型参数内存（与参数大小相同）。  


在神经网络推理时，一般不会保留梯度，因此内存消耗主要来自模型参数，以及即时的激活值。（因为不需要构建计算图，因此中间结果可以被释放）

推理时内存占用公式（简化版）：  
\[
\text{总内存} \approx \text{模型参数内存} + \text{即时激活值}
\]  
其中：  
- 模型参数内存 = `参数量 × 每个参数字节数`（通常float32占4字节）。  
- 即时激活值内存 ≈ `batch size × 当前所处隐藏层激活数 × 4字节`。  

---
**题目内容**  

假设你有一个简单的全连接神经网络：  
- **输入层**：1000维  
- **隐藏层**：4层，每层512个神经元  
- **输出层**：10维  
- **Batch Size**：64  
- **数据类型**：float32（每个数占4字节）  

**问题**：  
1. 计算模型的**总参数量**（公式：每层参数量 = 输入维度 × 输出维度 + 输出维度）。  
2. 估算训练时**前向传播**的中间激活值总数量
3. 计算当 `batch size=64` 时，训练过程占用的**最大内存**（单位MB，1MB = 2^20字节）。其中，哪个部分占大头？
4. 计算当 `batch size=64`时，推理过程占用的**最大内存**（单位MB）。 其中，哪个部分占大头？
5. 如果 `batch size` 增加到1024，训练内存占用会变为原来的多少倍？
6. 如果 `batch size` 增加到1024，推理内存占用会变为原来的多少倍？  
7. 自行查阅在面对受限的显存时，如何优化模型内存占用来支持训练？

## 实验五
按照d2l.ai的教程，自行学习CNN,RNN等模型的训练和推理。

## 实验六
了解现在的两大主流生成模型：自回归模型和扩散模型

# 更多
实践出真知！